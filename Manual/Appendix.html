<HTML><HEAD><META NAME="GENERATOR" CONTENT="Internet Assistant for Microsoft Word 2.0"><TITLE>Appendix</TITLE></HEAD><BODY><a name="top"></a><H1 ALIGN=CENTER> APPENDIX </H1><P>Contents of Chapter.<BR>	<ul>	<li><A href="#A1"> The Pascal Headers </a>	<li><A href="#A2"> &quot;Standardless&quot; Analysis </a>	<li><A href="#A3"> User Interfaces </a>	<li><A href="#A4"> Make File of n Noise Spectra </a>	<li><A href="#A5"> Gaussian and Lorentzian Distributions </a>	<li><A href="#A6"> Measure Beam Energy </a>	<li><A href="#A7"> Hardware Interfacing </a>	<li><A href="#A8"> F Keys and Keyboard Shortcuts </a>	<li><A href="#A9"> Detector Window Transmission </a>	<li><A href="#A0"> Background Filtering </a>	</ul><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A1"> APPENDIX I.  The Pascal Headers </A></H2><P>CONST<BR> Maximum_Channels = 8192;<BR> MaxNumPks = 50;         { Maximum peaks in a FIT, or window}<P>TYPE<BR> Spectrum_counts  = ARRAY [1..Maximum_Channels] OF Real; <BR>Experiment Header;<BR>           { The following record, Expt_InfoRec, comprises theEXPERIMENT header. }<P><A NAME="E_I">Expt_InfoRec</A> = RECORD  <BR> LastSpect : Integer;    <BR> FirstSpec : Integer;  { first Spectrum# }  <BR> Specimen_ID : String[50];;<BR> MCA_Filename  : String[25];;<BR> Specimen_Comment_Field : Str255;    <BR> Was_PassWord : String[25];  <BR> RefFile: Boolean; <BR> Analyst : String[50];<BR> Detector : <A href="#D_R">Detector_Rec</A>;<BR> Azimuth  : real;    { degrees }<BR> Elevation  : real;    { degrees }<BR> Detector_Area : real;    { cm sq }<BR> Detector_Thickness : real;    { mm }<BR> C_Thickness  : real;    { um }<BR> Diamond  : real;    { spare }<BR> Mylar : real;    { um }<BR> BN_Thickness : real;    { um }<BR> SiN_Thickness : real;    { um }<BR> Ice_Thickness : real;    { um, as pure oxygen }<BR> Au_Thickness : real;    { um }<BR> Al_Thickness : real;    { um }<BR> Be_Thickness : real;    { um }<BR> Si_Thickness : real;    { um }<BR>Moxtek : real;    { um }<BR> Paralene: real;    { um }<BR>WDS_Resolution: real;    { eV }<BR>dE : real;<BR> Si_Resolution  : real;    { eV at Mn K alpha }<BR> Energy_Intercept  : real;    { y intercept in eV }<BR> Energy_Slope : real;    { }    <BR> Number_of_Channels : integer;       <BR> kV  : real;<BR> Detector_Tilt : real;    { Beta }<BR>Quantum : real;<BR>Spare2 : Boolean;<BR>Spare3 : integer;<BR> END;<P><A NAME="D_R">Detector_Rec</A> = RECORD<BR>Spec : (EDS,WDS,EELS);<BR>ID : integer;<BR>END;<P>{The SPECTRUM header is made up by combining the following records.}<P>Specrtrum Header;<P> Element_InfoRec = RECORD<BR> Atomic_number : Integer;<BR> spare1 : real;  <BR> Weight_Fraction  : real; <BR> spare2 : real;  <BR> Valence : real;<BR> END;  <P> <A NAME="P_I">Plot_InfoRec</A> = RECORD<P> Plot_Connected : Integer;<BR> Plot_Symbol : Integer;<BR> Spectrum_Color  : RGBColor; {Red, Green, Blue }  <BR> END;<P> <A NAME="A_I">Acq_InfoRec</A> = RECORD<BR> Probed_Area  : real;   <BR> X_Position  : real;    { um }<BR> Y_Position  : real;    { um }<BR> Spare1 : real;<BR> FirstChannel : integer;<BR>LastChannel : integer;<BR> Begin_Faraday : real;    { nA }<BR> End_Faraday : real;    { nA }<BR> Begin_Time : Longint;  <BR>FirstValue : real;<BR>EndValue : real;<BR>spare2 : integer;<BR> Real_Time : real; { The time on the wall }<BR> Live_Time : real;  <BR> Slow_Channel_Counts : Longint; { Represents total out counts }<BR> Medium_Channel_Counts : Longint; { If UTW. Approx. input counts below 1 keV }  <BR> Fast_Channel_Counts : Longint; { Represents total input counts above 1 keV }     <BR> RequestedLiveTime : LongInt;  <BR> ActualLiveTime : LongInt;     <BR> Acquiring : Boolean;<BR>LLD : Integer; { Acquisition setup dialog box }<BR>Offset : Integer; { Acquisition setup dialog box }<BR>PulseProcessorType : Integer;<BR>PulseProcessorSetting : Integer;<BR> END;<P> <A NAME="S_I">Spectrum_InfoRec</A> = RECORD    <P> Spectrum_Type : String[4];  <BR> Spectrum_Comment_Field : Str255;<BR> Spectrum_Number : Integer;   <BR> Spectrum_Class : String[25];<BR> Theoretically_Generated : Boolean;<BR> This_is_a_Standard : Boolean;<BR> BkgSubtracted : Boolean;<BR> Maximum_Counts  : real;<BR> Minimum_Counts  : real;<BR> X_Tilt : real;    { degrees }<BR> Y_Tilt : real;    { degrees }<BR> Take_Off_Angle : real;    { degrees }<BR> Spec_Detector_Distance : real;    { mm }<BR> Spare : real;      <BR> Specimen_Thickness : real;    { cm }    <BR> Specimen_Density : real;    { g/cm sq. }<BR> Number_of_Elements : Integer;<BR> Element_Info : array[1..15] of Element_InfoRec; <BR>extra_space : array[1..157] of real;<BR>WDS_in_eV : Boolean;<BR>bool2 : Boolean;<BR>Average_Z : real;<BR>Spare1 : real;<BR>Spare2 : Boolean;<BR>Spare3 : Integer;<BR> END;<P>Element_InfoRec = RECORD<BR>Atomic_number : Integer;<BR>Spare1 : Integer;<BR>Weight_Fraction : real;<BR>Spare2 : real;<BR>Valence : real;<BR>END;<P>{In the next two RECORD definitions, we combine the above recordsinto the final Work_Spectrum RECORD. The &quot;Results&quot; spectrum,as well as the &quot;scratch&quot; spectra 1-8, are copies ofthe Work_Spectrum RECORD.}<P>Spectrum_Structure = RECORD<P> Spectrum_Info : <A href="#S_I">Spectrum_InfoRec</A>;<BR> Acq_Info : <A href="#A_I">Acq_InfoRec</A>;<BR> END;<P> Work_Spectrum = RECORD<BR> Expt_Info : <A href="#E_I">Expt_InfoRec</A>;<BR> Plot_Info : <A href="#P_I">Plot_InfoRec</A>;<BR> SpectrumStuff : Spectrum_Structure;   <BR> S : Spectrum_counts; { ARRAY [1..Maximum_Channels] OF Real }<BR> END;<P>A DTSA file of spectra will contain one Expt_InfoRec RECORD followedby as many Work_Spectrum RECORDs as there are &quot;spectra&quot;.<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A2"> APPENDIX II.  &quot;Standardless&quot; Analysis </A></H2><P>Virtually all modern energy dispersive x-ray analytical systemsinclude in their quantitative analysis procedures an option referredto as &quot;standardless analysis&quot;.  The &quot;standardless&quot;procedure offers a seductive lure to the analyst, particularlythe novice, in the form of the ultimate simplicity of procedures. Just record the spectrum of the unknown, without concern forthe electron dose as long as the deadtime is acceptable, supplythe correct beam energy and the elements to be analyzed, pusha button, and seconds later an analysis appears.  Encouragingly,the analysis totals exactly 100%!  As with most things in life,you get what you pay for. For careful application of the formalmethods of quantitative analysis by the &quot;ZAF&quot;, &quot;f(rhoz)&quot;,or the Bence-Albee (Ziebold-Ogilvie) empirical method with unknownsmeasured against known standards, the reward is analytical performancedescribed by an error histogram with a deviation of just a fewpercent.  The penalty for the apparent ease of the &quot;standardless&quot;approach is the real possibility of very large errors lying faroutside these comfortable bounds. <P> The underlying assumption of the &quot;standardless&quot; analysisapproach is the belief that we can calculate the intensity measuredfrom standards with accuracy equal to measuring them directly,based on certain properties of the single spectrum obtained froman unknown.  The &quot;standardless&quot; approach makes use offour equations to predict the x-ray intensity that would be obtainedfrom a pure element standard.  The first is the equation thatdescribes the absolute generation of x-rays. The second is anexpression to correct the loss of x-ray production because ofbackscattering, the R-factor.  The third is a correction for absorptionin the putative standard, the absolute absorption factor equation. The fourth is a calculation of the absolute efficiency of thespectrometer. Normalization is then used to force the resultsto apparent consistency.  <P> It is useful to consider how well the factors incorporated intothese equations are known:<P>[1] Absolute x-ray generation: The critical factors are the ionizationcross section, Qi, and the fluorescence yield, w.  While the productQiw is reasonably well known for the K-family of elements withatomic numbers from 20 - 32, the product becomes uncertain forlower atomic numbers.  The product is not well known for L- andM- family x-rays.   When an analysis must involve x-ray measurementsinvolving more than one family, inter-family calculations areparticularly difficult.  Some approaches attempt to rectify thisproblem by tying the calculation to specific experimental measures,e.g., Cu K - Cu L or Au L - Au M, determined at a typical operatingcondition, e.g., 20 keV.<P>[2] The absolute value of the R-factor is reasonably well known,probably within 5%.<P>[3] The absolute value of the absorption factor is also reasonablywell known, probably within 5% for lines above 3 keV, but progressivelyless accurate below 3 keV, and very poorly known below 1 keV.<P>[4] The greatest problem probably arises when an absolute calculationof spectrometer efficiency is needed over a wide energy range. The efficiency is reasonably smooth and predictable above 3 keV. Below 3 keV, the efficiency changes rapidly due to absorptionin all the components.  Comparing 0.5 keV with 3 keV, the efficiencytypically drops by a factor of 5 to 10.  Moreover, this is theregion of the spectrometer response where &quot;spectrometer aging&quot;effects, i.e. , changing efficiency with time in service, aremost strongly manifested.  The inevitable build-up of ice, evenin detectors with protective windows, causes substantial changesin the detector efficiency in this region.  It is possible withmost older (a few years old) detectors and many new ones to experiencesubstantial loss of counts from x-ray peaks due to incompletecharge collection. The magnitudes of loss are at best about onepercent for peaks at any energy, to as many as 20 percent forpeaks in the range of about 1.8-3 keV. Thus, a &quot;standardless&quot;analysis that involves soft x-ray lines in the 0.5 - 2 keV rangeand hard x-ray lines in the 5 - 10 keV range are likely to bein substantial error. <P> When &quot;standardless&quot; analysis is touted, typically theanalysis of stainless steel analysis is demonstrated.  Roundingup the usual suspects in steel analyses typically involves measuringthe transition elements Fe, Cr, Ni, Mn, Co, and V.  The K-linesof these elements span the range 5.0 - 7.5 keV where the spectrometerefficiency is virtually constant and close to unity.  The productis well behaved, and a reasonably accurate analysis can be obtained,especially if the procedure has been &quot;optimized&quot; forsteel analyses.<P>The summation to 100% is, of course, an artifact of the normalizationprocedure, and this total means nothing for the analyst's peaceof mind, and in fact, should reduce peace of mind.<P> Even with a &quot;standardless&quot; analysis that appears towork for steels, it is interesting to examine the effect of changingthe beam energy.  If the procedure is robust, it ought to workover a reasonable range of beam energies for analysis, e.g., 15- 25 keV.  The following two figures show an example of such a&quot;standardless&quot; analysis for specimens measured at differentbeam energies.  The SRM concentrations are very close to the nominalvalues 20%, 40%, 60%, and 80% so that the distribution of errorsfrom the &quot;standardless&quot; analysis is uncomfortably large.<P ALIGN=CENTER><IMG SRC=../images/Stdless1.gif HEIGHT=281 Width=377><P ALIGN=CENTER><IMG SRC=../images/Stdless2.gif HEIGHT=281 Width=377><P>It is obvious that by choosing the &quot;right&quot; kilovoltagethat we can get close to the correct answers. In this case 18keV appears to minimize the errors. However, if we do not pickthe right kilovoltage, we can generate some serious errors. Thistype of result can be expected from any &quot;standardless&quot;analysis procedure. Caveat emptor.<P> When multiple x-ray families must be brought to bear, the situationbecomes worse.  The above figures show the results of the analysisof NIST Standard Reference Material gold-copper alloys, that canonly be analyzed by taking combinations of lines that mix familiesand/or span a large difference in x-ray energy, e.g., Cu K (8.04keV) vs. Au L (9.71 keV) or Au M (2.12 keV).  Here the errorsare really unacceptably large.<P>The foundation for achieving accurate quantitative electron probemicroanalysis, whether by &quot;ZAF&quot;, &quot;f(rhoz)&quot;,or the Bence-Albee (Ziebold-Ogilvie) empirical method, rests uponthe standardization step, which consists of measuring an elementin the unknown against that same element in a standard under identicalconditions of beam energy, dose (current x livetime), spectrometerefficiency, and choice of x-ray line.  By forming the k intensityratio as a ratio between the same type of radiation, several importantterms in the expression for absolute x-ray intensity, and mostimportantly, the spectrometer efficiency, cancel in the ratio. It simply is not worth the apparent convenience of the &quot;standardlessapproach&quot; to concede the accuracy of the robust methods. Additionally, the loss of meaning in the analytical total of&quot;standardless&quot; analysis will inevitably hide those situationswhere an unexpected constituent appears at the analyzed location,or the specimen differs significantly from ideal bulk materialbecause of voids, surface relief, etc.<P>We summarize with a few suggestions on how to test (read as &quot;break&quot;)a &quot;standardless&quot; procedure, based on the above observations.Use a material that contains light and heavy components, suchas Cu-Au. Use X-Ray lines from different families such as K andM. Try the analysis at several voltages such as 15 kV and 25 kV.Try the analysis at low and high count rates, and try taking andprocessing the data at several x-ray take-off angles. Any of theabove should produce the same, correct, answers. &quot;Standardless&quot;analysis has a place in the laboratory. What we are warning thereader to beware of is the temptation to be lured into complacencyand to report the data as &quot;quantitative&quot;. &quot;Standardless&quot;analysis is simply more convenient than quantitative analysis.<B>It absolutely is a semi-quantitative tool and is not to be mistakenfor one that is quantitative.</B><P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A3"> Appendix III. User Interfaces </A></H2><P>This release of DTSA provides 1 menu item for interfacing userwritten quantitation functions into the DTSA framework. This itemis an interface to the well-known CITZAF (California Instituteof Technology ZAF) program written in BASIC by John Armstrongand converted to Pascal and made into a Macintosh program by ChuckFiori and John Armstrong. The program will do all standard algorithmsincluding several particle analysis corrections, most known ZAFcorrections, most known Phi(RhoZ ) corrections and a Bence Albeecorrection for geological specimens.<P><H2 ALIGN=CENTER><A NAME="A4"> Appendix IV. Make File of n Noise Spectra </A></H2><P>The microprobe assay of a specimen must provide both a mean andthe variance about this mean. Since we are concerned with microanalysis,the mean refers to the estimate of the weight or atom concentrationat a single analytical point, or some local grouping of points,from a homogeneous region of the specimen. The variance aboutthis mean then represents the uncertainty due to counting statisticsplus those aspects of the data reduction procedure that will contributeuncertainty such as peak unraveling and continuum suppression.The accuracy  of the estimate is a measure of the closeness ofour estimate to the true value of the concentration. The taskof predicting the variance about this estimated concentrationcan range from easy to quite difficult. As noted in a previoussection, as the specimen is further examined at many points, anyvariance greater than that determined above will represent truecompositional variation.<P>As with all measuring devices, the energy dispersive x-ray analysissystem has, for a given set of conditions, a sensitivity thattranslates into a minimum concentration of analyte that can bereported with a certain level of confidence (MDL). The estimationof MDL can also range from easy to quite difficult. We affectthe MDL by choices of experimental conditions such as the operatingpotential, the analytical x-ray lines used for analysis, choiceof data reduction algorithm, etc.<P>This section will discuss a procedure to estimate the MDL accuratelyfor a trace analysis and to estimate the variance accurately abouteach concentration in an analysis that encompasses a range ofconcentrations from minor to major element. The procedure willwork when either of the above situations does not lend itselfto treatment by conventional statistical methods, a frequentlyoccurring situation. We will begin by describing those analyticalconditions whereby standard statistical methods are difficultto apply for the determination of MDL and concentration variance.<P>As with the WDS, a spectrum observed with an energy dispersivespectrometer (EDS) consists of x-rays arising from both the characteristicand the continuum process. The x-ray peaks arising from the characteristicprocess contain the analytical information we seek. Often thepeaks we wish to determine will overlap with the peaks from otherelemental constituents of the specimen. Furthermore, the peaksare always superposed onto a smoothly varying spectrum of x-raysarising from the continuous process, and both the characteristicand continuum signals are modulated by the effects of countingstatistics. <P>The MDL and variance about a measured concentration depend onthe magnitude of the peak and background intensities, the degreeof peak overlap,  and the algorithms used to extract the requiredpeak intensity and background intensity values below the peak.In general there is no straightforward way of estimating the quantitiesrequired for a standard statistical treatment as discussed inthe previous sections. Therefore, some analysts, when faced withthe problem of providing good error estimates, will resort tothe time consuming but extremely reliable technique of  directmeasurement. In this method the specimen is sampled n times eachat a number of representative locations. For each of the n replicatemeasurements at each location, one will go through all the spectralprocessing (removal of background and peak overlap effects) anddata reduction (such as ZAF) steps required to arrive at an elementalconcentration. From the n results at each location, the analystcan then predict by conventional statistical methods the expectedvariance for each of the elemental concentrations at the variouspresumably representative locations.  Knowing, then, the expectedvariances, the analyst can proceed with a strategy of single measurementsat each analytical point in the specimen. For specimens with manyphases or a wide range of compositions, this procedure can bequite daunting. <P>DTSA can generate from first principles an x-ray spectrum thatwill be more than sufficiently close in all the germane physicaland statistical properties to represent an actual spectrum froma real specimen.  From this generated spectrum, one can then deduceaccurate estimates of the variance about mean compositional values.One may also accurately estimate the MDL of any analyzable stableelement in any stable matrix without the need to produce a setof calibration standards. Furthermore, one may adjust the experimentalparameters to determine the optimum ones that will produce thelowest  MDL. All of this can be done relatively rapidly, usingonly a small computer, before even presenting a specimen to theelectron beam.<P>One can enter a hypothetical set of concentrations and analyticalconditions, and generate a &quot;spectrum&quot; that will havethe correct number of counts for the entered conditions. Thisspectrum will not have counting &quot;noise&quot; on it and representsthe true &quot;mean&quot; spectrum. One can add counting noiseand then go through all the procedures involved in extractingnet peak areas and conversion of these to elemental concentrations.By repeatedly adding new counting noise to the same &quot;mean&quot;spectrum and repeating all the steps to get a concentration foreach of the resulting &quot;spectra&quot;, one can obtain a setof &quot;measurements&quot; from which it is possible to determinewith a high degree of certainty the MDL and variance about a concentrationmean for an element in a given chemical matrix. The variance term,is, of course, what is required to derive the statistical quantity&quot;sensitivity&quot;. <P>This procedure of adding new noise to a &quot;mean&quot; spectrumis merely an application of the &quot;Monte Carlo&quot; methodin statistics which is becoming very popular with the advent ofpowerful and inexpensive computers, and is very easy to do inDTSA.<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A5"> Appendix V.. Gaussian and Lorentzian Distributions </A></H2><P>The energy distribution of characteristic x-rays of a single lineis well described by the Lorentzian probability distribution:<P ALIGN=CENTER><IMG SRC=../images/Eqn1.gif HEIGHT=70 Width=123><P>where Yi is the amplitude in the ith channel, g = FWHM&#247;2, AC is the amplitude at the center energy of the peak, EC is thecenter energy, and Ei is the energy of the ith channel away fromthe center energy.<P>The peak shape of a characteristic x-ray line, even if it assumedto be a perfect step function (i.e., infinitely narrow), as observedby an energy dispersive system is well described by a Gaussiandistribution:<P ALIGN=CENTER><IMG SRC=../images/Eqn2.gif HEIGHT=45 Width=166><P>where, again, Yi is the amplitude in the ith channel, g = FWHM&#247;2, AC is the amplitude at the center energy of the peak, EC is thecenter energy, and Ei is the energy of the ith channel away fromthe center energy. The ln(2) term is a scaling constant to permitthe use of FWHM&#247;2 in the equation, rather than the usual&quot;standard deviation&quot;, s found in statistical applications.The Gaussian distribution is a good approximation for many  instrumentalresponse functions and is often used to describe the peak shapesobserved by Si and Ge detectors. In reality there is a small distortionon the low energy side of the peak due to incomplete charge collection,but this is often neglected. <P>The relative shape of these two distributions, for the same amplitude,energy and width, is shown in the following figure:<P ALIGN=CENTER><IMG SRC=../images/Gau_Lor.gif HEIGHT=347 Width=420><P>Notice the relatively long tails of the Lorentzian distribution.These tails can extend a considerable distance (up to 1 keV) andare the reason that background correction is important in WDSwhen performing a trace element analysis using an x-ray line inthe vicinity of a large line from another element in the specimen.Note also that the Lorentzian distribution is narrower at thetop than a Gaussian and is therefore better able to register thepresence of the Ka2 peak on the left side of the distribution.<P>The instrumental broadening of the energy dispersive detectoris large. Typically the FWHM at the energy of Mn Ka is 135-165eV, while the natural width at Mn Ka1,2 is just a few eV. Consequently,the Gaussian shape of the energy dispersive detector dominatesthe Lorentzian shape of the natural x-ray line and the resultingobserved peak appears to be purely Gaussian (and much wider thanin the above figure).<P>For the case of a wavelength dispersive spectrometer, with itsconsiderably better energy resolution, the shape of the naturalline width cannot be neglected. Often, the peak shapes as observedby a WDS have strong Lorentzian contributions. Stated mathematically,the observed peak shape is the convolution of the natural lineprofile and the instrumental response function. In the case ofthe WDS, the instrumental response curve itself has a Lorentziancontribution due to deliberately induced imperfections (polygonization)in the structure of several types of diffracting crystals. Onelast complicating factor is the effect of chemical bonding onthe natural x-ray line distributions that will distort the Lorentzianprofile. This last effect is most pronounced for the Ka x-raylines from Be-Si. <P>In general, the instrumental response curve for a wavelength dispersivespectrometer is not predictable. It can change dramatically dependingon the choice of diffracting crystal, slit width, etc. The shapecan vary from near Gaussian, for the new synthetic multilayerdevices, to strongly Lorentzian, for a Lithium Fluoride crystal.<P>It is possible, however, to convolve a given quantity of Lorentzianwith a given quantity of Gaussian, and provide an instrumentalresponse curve that very closely matches the observed distributionfrom a given wavelength dispersive spectrometer. The relativeproportion of these two contributions is called the Hypermet numberin the WDS dialog. This dialog is obtained if the WDS radio buttonis chosen and the Detector Parameters button is pushed. <P>The SIMPLEX procedure has the option of fitting WDS peak shapesusing a user provided Hypermet number. Once this number is determinedfor a given element on a given spectrometer, it should remainconstant for long periods of time. Accurate determination of peakareas from a WDS is only important for a few elements, i.e., theabove Be-Si K lines. In many laboratories, it may be necessaryto determine a Hypermet number for only oxygen. This matter isdiscussed in more detail in the SIMPLEX section.<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A6"> Appendix VI. Measure Beam Energy </A></H2><P>Recently, there has been much interest in analyzing materialsusing lower beam voltages. The correction procedures use the so-called&quot;overvoltage&quot; term (beam energy/excitation energy).Uncertainties in the beam voltage will generate larger errorsat low overvoltage than at high overvoltage. The user may be facedwith an instrument that does not have a very precise voltmeterto measure the beam voltage, or the reported voltage may not representthe potential drop from the filament to ground (specimen). Thetrue beam energy may differ from the measured potential drop byseveral hundred volts.<P>It is difficult and potentially exciting to measure the beam voltagewith a calibrated voltmeter. However, the physics of the beam-specimeninteraction provides a method that is built into the energy-dispersivex-ray detector. Continuum x-rays are generated up to the incidentenergy of the electron beam (the Duane-Hunt limit). The beam voltagecan then be determined with a calibrated energy dispersive spectrometer(EDS) by examining the high energy portion of the spectrum. Whilean approximate answer can be obtained by simply looking at thespectrum, the action of the detector response function distortsthe apparent Duane-Hunt limit from the true value. DTSA uses amethod that will provide consistently unbiased values.<P>The intensity of the continuum x-rays at a given energy E  canbe described with an equation derived by Kramers: <P ALIGN=CENTER><IMG SRC=../images/Eqn3.gif HEIGHT=34 Width=112>                        1<BR><P>Where Z is the atomic number, k is a constant, E0 is the beamvoltage, dE is the energy interval and I(E)dE is the x-ray intensityper energy interval. A plot of the high energy end of the x-raycontinuum of copper is shown in Figure 1. <P ALIGN=CENTER><IMG SRC=../images/DH_limit.gif HEIGHT=355 Width=430><P>Figure 1. A segment of a synthetic copper spectrum measured withthe beam voltage set at a nominal value of 15 keV. The solid lineis the generated x-rays computed with the above equation. Thedashed line is the generated x-rays convolved with the Gaussiandetector function. The data have all been transformed back tointensity by dividing by E.<P>The solid line represents the generated x-rays as defined by equation1, and crosses the x-axis at exactly the energy of the electronbeam, E0. When this continuum is convolved with the Gaussian detectorfunction, the dashed line in Figure 1 is produced, that interceptsthe x-axis about 100 eV higher in energy than the original expression.This distorts the shape of the curve at the Duane-Hunt limit makingit more difficult to fit the data accurately. Because of the 1/E term in equation 1, the plot of intensity versus energy is slightlycurved near the Duane-Hunt limit (Figure 2 is a plot of the continuumtails for carbon, copper and gold)<P ALIGN=CENTER><IMG SRC=../images/DH_synspec.gif HEIGHT=348 Width=432> <P ALIGN=CENTER><IMG SRC=../images/DH_quad.gif HEIGHT=346 Width=423><P>Figure 3. A segment of an experimental spectrum of copper at anominal beam voltage of 15 keV together with a least squares fitto a second degree polynomial. The data have all been transformedback to intensity by dividing by E. The polynomial fit is stronglyaffected by the pile-up events.<P ALIGN=CENTER><IMG SRC=../images/DH_Cufits.gif HEIGHT=373 Width=434><P>Figure 4. A segment of an experimental spectrum of copper at anominal beam voltage of 20 keV together with a least squares fitto a second degree polynomial (dashed line) and a fit using therobust estimation method described here (solid line). The datahave all been transformed back to intensity by dividing by E.Note that the polynomial fit never crosses the horizontal axis.<P>In Figure 3, the pulse pile-up above the Duane-Hunt limit tendsto pull the intercept to a higher energy and even though the quadraticfit shown in Figure 4 appears to be a good model of the data,it only produces imaginary roots.<P>Another difficulty to be reckoned with  is the statistical noiseassociated with the x-ray spectrum. For the most part, this noisecan be modeled by a normal distribution however, near the Duane-Huntlimit the normal distribution is not valid. A basis for the methodof least squares is that the data be normally distributed. Therefore,attempting to use a method for the modeling of normally distributeddata to model a Poiss&oacute;n process would be mathematicallynaive, since the data go from relatively large numbers, wherethe normal approximation is valid, to zero.<P>DTSA uses a &quot;robust estimator&quot; to fit a straight lineby minimizing the sum of the absolute deviations (instead of theusual sum of the squares of the deviations, which is inappropriatein this application). The function to be minimized is therefore<P ALIGN=CENTER><IMG SRC=../images/Eqn4.gif HEIGHT=40 Width=77>                  2<BR><P>instead of c2 as required for least squares. Figure 7 is a dramaticexample of the effectiveness of the robust estimator, where thepresence of the gold peak does not significantly influence thequality of the fit through the continuum.<P>If we multiply the x-ray intensities in equation [1] by theirenergies, the result should be a straight line, the x-interceptof which is the Duane-Hunt limit. Experimentally, a slight deviationfrom a straight line arises because of imperfections in Kramersequation and the Gaussian response shown in Figure 1. Becauseit is physically impossible to have non-zero intensities beyondthe Duane-Hunt limit, the next step is to eliminate all the databeyond the root from the fitting procedure and refit the data.An iterative procedure has been devised to  eliminate systematicallydata points with energies higher than the value of the root ofthe equation. The procedure always uses 300 channels (a channelis defined as an energy interval and is usually 5, 10 or 20 eV)on the low energy side of the best estimate of the root and, onthe high energy side, 150 channels, which on successive iterations,is reduced to 50 channels, then 10, 7, 5, 3 and finally 1 channel.The procedure is not allowed to terminate until after the numberof channels on the high energy side is reduced to 1. The resultingexpression is used to compute predicted values of the intensities,which are then plotted with the measured values (Figures 5-7).<P ALIGN=CENTER><IMG SRC=../images/DH_Cfit.gif HEIGHT=330 Width=424><P>Figure 5. A segment of an experimental spectrum of carbon at anominal beam voltage of 15 keV together with a fit using the robustestimation method described here. The data have all been transformedback to intensity by dividing by E.<P ALIGN=CENTER><IMG SRC=../images/DH_Cu15.gif HEIGHT=343 Width=425><P>Figure 6. A segment of an experimental spectrum of copper at anominal beam voltage of 15 keV together with a fit using the robustestimation method described here. The data have all been transformedback to intensity by dividing by E. Note that the pileup eventsdo not affect the fit.<P ALIGN=CENTER><IMG SRC=../images/DH_Au15.gif HEIGHT=333 Width=430><P>Figure 7. A segment of an experimental spectrum of gold at a nominalbeam voltage of 15 keV together with a fit using the robust estimationmethod described here. The data have all been transformed backto intensity by dividing by E. Note that the gold peaks have verylittle effect on &quot;pulling&quot; the background fit higher.This is a good example of the power of a &quot;robust estimator&quot;in this kind of application where conventional least squares isinappropriate.<P>To test the fitting method, energy dispersive spectra are synthesizedwith equation 1 so that the &quot;true&quot; Duane-Hunt limitcan be defined. For this test case, the proposed fitting methodis exact. When specimen absorption, detector efficiency, spectrometerbroadening and Poiss&oacute;n noise are added to a synthesizedcopper spectrum, the fit is still within 0.33% of the &quot;true&quot;value for 10 repeated syntheses with random noise.<P>An examination of Figures 2, 5, 6 and 7 demonstrates why the resultsare better with high atomic number targets than with low. Sincethe continuum intensity increases with atomic number, the higheratomic number targets provide more x-ray counts in the spectrum,and therefore better statistics. Also, the higher number of countsmakes the method more robust, as demonstrated by the fit shownin Figure 7. Here, two of the gold L-series x-ray lines occurwithin the range of the data to be fit. Note that the fitted lineis still a good estimation of the data even though the gold peaksare present. In fact, if the peaks are stripped from the spectrumand the fit is repeated, the Duane-Hunt limit is only loweredby 4 channels (40 eV).<P>As with any procedure of this type, the results depend on theinformation supplied by the user. The spectrum should be accumulatedfor a sufficient length of time and at a low enough detector dead-timeto ensure good statistics and to minimize any pulse pile-up. Itis necessary that the detector be properly calibrated to avoidmiscalibration errors propagating through to the final result.Further testing of the method will be carried out to identifyany possible systematic errors that might arise from the deviationin linearity from the assumed response given by equation  1.<P>One last warning: since the procedure currently uses 300 channels of data for fitting, attempting to fit spectra collected at low beam voltages (less than 5 keV) may produce errors. We will examine this area and try to make the procedure work at these lower voltages.<P> <A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A7"> Appendix VII. Hardware Interfacing </A></H2><P>Hardware Interfacing of Spectral Acquisition; to DTSA with aMacintosh CDEF<P>The following figure is the interface into the acquisition capability.The two rectangular boxes that contain &quot;0&quot; are actuallya combination analog and digital count-rate meter. As the count-ratesfrom the fast and slow channels (Input and Output) change theboxes will move up or down (the analog part) and the number inthe box will show the digital value. <P ALIGN=CENTER><IMG SRC=../images/meter.gif HEIGHT=123 Width=87><P>All hardware functions are controlled from the Acquire Menu. The items in this menu are designed to collect data from more than one detector.<P ALIGN=CENTER><IMG SRC=../images/Acq_menu.gif HEIGHT=230 Width=157><P>The "Assign Display(s)" item allows the user to select into which of the ten available displays a spectrum will be accumulated and displayed on the screen. It is no longer necessary to accumulate into the "WORK" display; however, you can only save a spectrum from "WORK".<P ALIGN=CENTER><IMG SRC=../images/Assign_disp.gif HEIGHT=130 Width=327><P>Select the detector from the pop-up menu and the display with the appropriate button.<P>If the "Hardware Set-up" item is invoked, then the following dialog will appear for each detector:<P ALIGN=CENTER><IMG SRC=../images/Acq_setup.gif HEIGHT=270 Width=461><P>This dialog permits, via the CDEF capability of the Macintosh,to pass the above information back and forth to an acquisitionNu-Buss card. <P>Spectra Acquisition can be interfaced to DTSA using the ControlManager and a custom control definition function (CDEF). Sourcecode for the dummy acquisition CDEF implemented in DTSA is includedbelow and the reader should be familiar with control definitionfunctions (see Inside Macintosh volume 1 chapter 10 &quot;ControlManager&quot;).<P>Currently the dummy acquisition control is accessed by a popupmenu that is activated by holding down the mouse button whileinside the boundary of the control. From this menu, one can select1 of the 4 following functions: Clear/Start, Stop, Continue andSet-up. One could use these functions to control the spectra acquisition.DTSA accesses the acquisition CDEF each time the main event loopis executed. The control is set to a value of five (AcqUpdate)to force entry into the CDEF code. A &quot;feature&quot; of theMacintosh operating system must be bypassed to guarantee entryinto the CDEF. If the value of the control is the same as thevalue requested, then the Mac OS will NOT call the CDEF. ThisCDEF gets around this problem by saving a copy of the currentvalue (or acquisition state as it would be) and setting the controlvalue to zero (AcqNull). When the CDEF is called with a valueof AcqUpdate, the CDEF should check the state of acquisition andif actively acquiring a spectra it should update the internalDTSA spectra structure &quot;WORK&quot;. The DTSA internal structureassociated with the &quot;Work&quot; spectra can be accessed witha handle that is stored in the control's ContrlData Handle whenthe control is created by DTSA. After processing the AcqUpdatemessage, if the CDEF requires the DTSA program to process theinformation transferred to the &quot;WORK&quot; structure, thenit should set the control value to one (AcqStart) to force anupdate, otherwise the value should be set to AcqNull. This updatewill update and redraw the input and output count rate controlsas well as the spectra and other information that has been transferredto the &quot;WORK&quot; structure.<P>Since a CDEF is a resource that is located in the resource forkof the DTSA program, one can use ResEdit or replace the existingdummy CDEF with a real CDEF that is capable of spectra acquisition.Remember that the CDEF needs to be the same number as the existingCDEF or DTSA will not  load it in place of the existing CDEF.An example CDEF is available from NIST on request. The examplecode is in Pascal. This can be compiled and linked to the programusing the Apple Macintosh Programmers Workshop (MPW).<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A8"> Appendix VIII. F Keys and Keyboard Shortcuts </A></H2><P>If the particular model Macintosh used by you has an extendedkeyboard with F keys and a numeric keypad, then it is possibleto use certain predefined keys to accomplish certain tasks. Theassignment of these keys is as follows:<P>Numeric keypad &quot;8&quot; will do a ScaleUp.<P>Numeric keypad &quot;2&quot; will do a ScaleDown.<P>F1 is a key which DTSA uses instead of the option key. Becauseof the way the Macintosh event loop is used in the program itwas not possible to use the option key as used in many other programs.The F1 key is used throughout DTSA to speed or slow the effectsof certain other keys or arrow icons. See the individual sectionsfor a description.<P>F2 will rotate work and results. Remember that a rotate will notrotate the headers.<P>F3 will bring up the &quot;periodic table&quot; for the x-raydatabase. <P>F4 will print the spectral display.      <P>F5 will Do a Fit.<P>F6 will Add A Fit.<P>F7 will See A Fit.<P>F8 will clear the work spectrum.<P>F9 will bring up Spectrum Header Dialog.<P>F10 will bring up the Experiment Header Dialog.<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A9"> Appendix IX. Detector Window Transmission </A></H2><P>   The radiation emitted from the specimen must pass through severallayers of &quot;window&quot; material before it arrives in the&quot;active&quot; part of the detector. The nominal purpose ofthe first window material is to protect the cooled detector chipfrom the relatively poor vacuum in the specimen chamber whichcan be high in water vapor and organic components because of frequentexchanges of the specimen. It is very important that these componentsdo not condense on the cooled detector. Even with a protectivewindow, ice will build up, over a period of time,  which willabsorb lower energy x-rays. Modern detectors have special heatersto remove this ice buildup, some of which actually originatesfrom the detector assembly and cryostat at the time of manufacture.This problem can be overcome by the use of high vacuum constructionpractices. Since many modern scanning and transmission electronmicroscopes themselves are ultra-high vacuum devices,  the purposeof the window is actually to protect the microscope vacuum fromthat of the detector.<P>   Protective windows are made from a variety of different materialseach having advantages and disadvantages. Historically, beryllium, about 7.6 mm thick (0.3 mils), has been used. This thicknesshas the advantage that it can withstand a full atmosphere pressuredifferential thereby avoiding possible shock associated with specimenchanges.  Furthermore, the detector can be removed from the microscopewithout the need for isolation valves. It is also opaque to opticalphotons, which can easily saturate the  detector during operation.About 1985, there was a shortage of 7.6 mm beryllium foil, dueto occupational health and safety regulations, and 12 mm  wasused for a year or so; many of these detectors still exist. Recently,protective window materials with considerably less mass-thickness,still able to withstand a full atmosphere of pressure differential,have gained wide popularity. They are constructed of thin filmson a support grid having approximately 85% open space. Low energyx-rays, such as oxygen or carbon, will only pass through the un-backedportions of the film, while more energetic x-rays (&gt;8 keV)can pass both through the film and the support grid. The gridis usually pure silicon or a metal such as nickel and the windowmaterials  are either boron nitride, silicon nitride, diamond,or are polymeric.  Examples of transmission curves for variouswindow materials currently in use are given in Figure 1.  Sincethin films of these materials are transparent and the detectoris also sensitive to visible light, it is either necessary toeliminate possible light sources or to apply an optically opaquelayer to the protective window.  Light sources include microscopeillumination systems, viewing ports, and light leaks; however,specimen cathodoluminescence is rarely intense enough to causea serious problem.<P>   The second window layer is an electrical contact (about 10-20nm and usually gold). Gold has been observed to form &quot;islands&quot;during the evaporation process and hence is not uniform in thickness.The third window layer consists of inactive p-type silicon extending100-200 nm or less into the detector. This layer is also not uniformand is often called the &quot;dead layer&quot; or &quot;silicondead zone&quot;. The combined effect of these Au and Si layersis often overlooked. In reality, their effect is as great, orgreater, than that of any of the new &quot;thin&quot; window materials.It is clear from Figure 1a that there really is no such thingas a &quot;Windowless&quot; detector. In this context, the abilityto see pure beryllium is misleading. This metal has very low continuumradiation (Z=4) and hence the beryllium K line seems to be large.To demonstrate this, generate and compare pure beryllium and berylliumin &Aring;90% copper matrix. It will be seen that the practicalmeasurement of Be is difficult at best.<P>   The chief utility of the detection of beryllium is that itdemonstrates that the EDS system is performing in a state-of-the-artmanner.<P ALIGN=CENTER><IMG SRC=../images/det_eff-1.gif HEIGHT=352 Width=453><P>Figure 1a. Transmission curve for  a &quot;Windowless&quot; detectorwith a 0.01 &#181;m Au contact and a 0.1 &#181;m silicon deadlayer. The energies of the light element K lines are shown alongthe energy axis. It should be noted that the mass absorption coefficientsfor energies less than 200 eV are poorly known.<P ALIGN=CENTER><IMG SRC=../images/det_eff-2.gif HEIGHT=354 Width=445><P>Figure 1b. Transmission curve for several commercially availablewindow materials. Thickness' and compositions were obtained fromadvertising literature or published values. The energies of thelight element K lines are shown along the energy axis. It shouldbe noted that the mass absorption coefficients for energies lessthan 200 eV are poorly known. The actual transmission of x-raysinto the active part of the detector is the combined effect ofparts &quot;a&quot; and &quot;b&quot; of this figure. This demonstratesthat a consideration of window materials alone results in an overlyoptimistic expectation of performance. <P> During passage of x-rays through all of the window layers, absorptionoccurs. It is important to recall that photoelectric absorptionrefers to a process in which x-rays are diminished in number,but not in energy, thus the energies of the observed spectrallines are not altered by the effects. In the case of 0.3 mil berylliumprotective windows, nearly all x-rays below about 600 eV are eliminateddue to absorption effects. Above 2 keV, virtually all x-rays aretransmitted. Between these limits, the absorption increases withdecreasing energy such that at 1.5 keV about 70% of the x-raysare transmitted, while for an energy of 1 keV, the transmissionis 45%. Absorption by the gold and silicon layers is much lesssignificant because of the small mass thickness of these layers.However, a noticeable change in the x-ray continuum is observedat the absorption edge of silicon and to a lesser degree at gold,Figure 1. Just above the energy of the absorption edge, the massabsorption coefficient increases abruptly, resulting in a decreasein the measured continuum x-radiation. The height of the resultingstep is an indication of the thickness of the layer. Note thatthe action of the broadening effect of the detection process causesthe absorption edge, which in reality is a sharp change of absorptionover a range of about l eV, to be smeared over a much broaderrange, typically 100 eV for the silicon absorption edge.<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P><H2 ALIGN=CENTER><A NAME="A0"> Appendix X Background Filtering using the Digital Filter </A></H2><P>In this method of background reduction, the continuum componentof an x-ray spectrum is viewed as an undesirable signal whoseeffect can be removed by mathematical filtering or modificationof the frequency distribution of the spectrum. Knowledge of thephysics of x-ray production, emission, and detection is not required.<P>Filtering techniques take advantage of the fact that the continuumcomponent of an x-ray spectrum is smooth and slowly varying, asa function of energy, relative to the characteristic x-ray peaks,except at absorption edges. If we mathematically transform anx-ray spectrum from energy space into frequency space, the resultwould be as shown in Figure 1. The horizontal axis gives the frequencyof an equivalent sine wave component. For example, a sine wavewith a full period of 0-10keV would be plotted at channel 1 whilea sine wave with a full period in 10 eV would be plotted at channel1000. The vertical axis |F| gives the population of each sinewave. In this representation, the continuum background that hasa long period variation is found at the low-frequency end. Thecharacteristic peaks vary more rapidly than the background andare found at higher frequency. The noise is found at all frequencies.<P ALIGN=CENTER><IMG SRC=../images/dig_filt-1.gif HEIGHT=180 Width=235><P>Figure 1. Frequency space (Fourier transform) representation ofelectron excited x-ray spectrum. (See text for explanation.)<P>We can now mathematically suppress the frequency components responsiblefor the slowly varying continuum and additionally suppress a portionof the frequency components responsible for the statistical noise.By then performing a reverse Fourier transform, we go back toenergy space with a spectrum that should now be comprised onlyof characteristic peaks devoid of continuum and that is, furthermore,&quot;smoothed&quot; for statistical variation. Unfortunately,close examination of Figure 1 reveals that the three main componentsof the transform overlap. Consequently it is not possible to suppressall the undesirable components without sacrificing part of thepeak components. Similarly, it is not possible to keep all thepeak components without including part of the undesirable components.The result in either case will be a spectrum with undesirabledistortions.<P>A filtering technique that is now widely used is the &quot;tophat&quot; digital filter. The top hat filter is a simple and elegantalgorithm-a fact that can easily be obscured by the mathematicalformalism required to describe it. Simply stated, the top hatfilter is a special way of averaging a group of adjacent channelsof a spectrum, assigning the &quot;average&quot; to the center&quot;channel&quot; of the filter, and placing this value in achannel in a new spectrum that we will call the filtered spectrum.The filter is then moved one channel and a new &quot;average&quot;is obtained. The process is repeated until the entire spectrumhas been stepped through. The filter in no way modifies the originalspectrum; data are only taken from the original to create a newspectrum. The &quot;averaging&quot; is done in the following manner.The filter, see Figure 2, is divided into three sections: a centralsection, or positive lobe, and two side sections, or negativelobes. The central lobe is a group of adjacent channels in theoriginal spectrum from which the contents are summed togetherand the sum divided by the number of channels in the central lobe.The side lobes, similarly, are two groups of adjacent channelsfrom which the contents are summed together and the sum dividedby the total number of channels in both lobes. The &quot;average&quot;of the side lobes is then subtracted from the &quot;average&quot;of the upper lobe. This quantity is then placed in a new spectruminto a channel that corresponds to the center channel of the filter.<P ALIGN=CENTER><IMG SRC=../images/dig_filt-2.gif HEIGHT=230 Width=293><P>Figure 2. Effect of top hat digital filter on spectrum comprisedof a Gaussian peak plus a sloped linear background. The filteredspectrum is plotted immediately below the actual spectrum. Thechannel correspondence for one calculation of the top hat filteris shown.<P>The effect of this particular averaging procedure is as follows.If the original spectrum is straight, across the width of thefilter, then the &quot;average&quot; will be zero. If the originalspectrum is curved concave upward, across the width of the filter,the &quot;average&quot; will be negative. Similarly, if the spectrumis curved convex upward the &quot;average&quot; will be positive.The greater the curvature, the greater will be the &quot;average.&quot;The above effects can be observed, for a Gaussian superposed ona linear background, in Figure 2. In order for the filter to respondwith the greatest measure to the curvature found in spectral peaks,and with the least measure to the curvature found in the spectralbackground, the width of the filter must be carefully chosen.In general, the width of the filter for any given spectrometersystem is chosen to be twice the full width at half the maximumamplitude (FWHM) of the MnKa peak, with the number of channelsin the upper lobe equal to or slightly more than the combinednumber of channels in the side lobes.<P>Because the top hat filter &quot;averages&quot; a number of adjacentchannels, the effects of counting statistics in any one channelare strongly suppressed. Consequently, in addition to suppressingthe background under spectral peaks, the digital filter also &quot;smoothes&quot;a spectrum. Note that in Figure 2, the top hat filter convertsthe sloped background into a flat background.<P>In conclusion, the effects of passing a top hat digital filterthrough an x-ray spectrum as recorded by a Si(Li) spectrometersystem are to (1) strongly suppress the background and statisticalscatter and (2) significantly alter the shape of the spectralpeaks. The result strongly resembles the smoothed second derivative;however, this distortion has no adverse statistical or mathematicaleffects of any consequence. Clear advantages of the method aresimplicity and that an explicit model of the continuum is notrequired. However, since the continuum has been suppressed, theinformation it carried (i.e., average atomic number, mass-thickness,etc.) is no longer available.<P><A HREF="#top">Top</A><P><A HREF="../DTSA_Manual.html">Table of Contents</A><P></BODY></HTML>